{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  制备训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# k折交叉验证\n",
    "from sklearn.model_selection import KFold,train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "def get_data(path):\n",
    "    with open(path,mode='rb') as in_file:\n",
    "        positive_c,negative_c=pickle.load(in_file)\n",
    "    return positive_c,negative_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordlist(txt_data, topnum, min_count, ratio_value):\n",
    "    word_count = Counter()\n",
    "    \n",
    "    for line in txt_data:\n",
    "        word_count.update(line)\n",
    "        \n",
    "    # print (word_count.most_common())\n",
    "    idCODES = {'<PAD>': 0, '<UNK>': 1 }\n",
    "    count = len(idCODES)\n",
    "    \n",
    "    i = 0\n",
    "    vocab2id=copy.copy(idCODES)\n",
    "    \n",
    "    for word,cnt in list(word_count.most_common()):\n",
    "        if i < topnum:\n",
    "            if(cnt>min_count):\n",
    "                vocab2id[word]=count\n",
    "                count+=1\n",
    "                i+=1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    id2vocab={vi:v for v,vi in vocab2id.items()}\n",
    "#     print(\"total word number:\",len(id2vocab)-len(CODES))\n",
    "    return vocab2id,id2vocab\n",
    "        \n",
    "# vocab_to_int, id2vocab=get_wordlist(positive ,100000,5,ratio_value=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 词袋模型，只考虑权重  内存不够\n",
    "def bow_data_prepare(alldata, vocab2id):\n",
    "    \n",
    "    data1 = np.zeros((int(1/2*len(alldata)),len(vocab2id)))\n",
    "    \n",
    "    for i in range(0, int(1/2*len(alldata))):\n",
    "        for word in alldata[i]:\n",
    "            if word in vocab2id.keys():\n",
    "                data[i, vocab2id[word]]+=1\n",
    "      \n",
    "    left = len(alldata) - int(1/2*len(alldata))\n",
    "    data2 = np.zeros(left,len(vocab2id))\n",
    "\n",
    "    for i in range(left, len(alldata)):\n",
    "        \n",
    "        for word in alldata[i]:\n",
    "            if word in vocab2id.keys():\n",
    "                data2[i-left, vocab2id[word]] += 1\n",
    "                \n",
    "    return np.concatenate((data1, data2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_data_prepare(sent_list,vocab2id):\n",
    "    data=np.zeros((len(sent_list),len(vocab2id)))\n",
    "    for i in range(len(sent_list)):\n",
    "        for word in sent_list[i]:\n",
    "            if word in vocab2id.keys():\n",
    "                data[i,vocab2id[word]]+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sent2id(all_train, word_num):\n",
    "    train = np.zeros((len(all_train), word_num))\n",
    "    \n",
    "    for i in range(0, len(all_train)):\n",
    "        k = min(len(all_train[i]), word_num)\n",
    "        for j in range(0, k):\n",
    "            if all_train[i][j] in vocab_to_int.keys():\n",
    "                train[i,j] = vocab_to_int[all_train[i][j]]\n",
    "            else :\n",
    "                train[i,j] = vocab_to_int['<UNK>']\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_int len: 8122\n",
      "train len: 19561\n",
      "all the preprocess done\n"
     ]
    }
   ],
   "source": [
    "# 导入分好词的数据\n",
    "positive,negative=get_data('12000_5_8.p')\n",
    "# 将list随机排序\n",
    "random.shuffle(positive)\n",
    "random.shuffle(negative)\n",
    "negative=negative[:len(positive)]\n",
    "# 将一个list拼接到另一个上\n",
    "positive.extend(negative)\n",
    "\n",
    "#获取单词转换表\n",
    "vocab_to_int,id2vocab=get_wordlist(positive, 100000, 5, ratio_value=0)\n",
    "print(\"vocab_to_int len:\",len(vocab_to_int))\n",
    "\n",
    "\n",
    "# 打标签，积极为1，消极为0\n",
    "data_label = np.zeros((len(positive),1))\n",
    "for i in range(len(data_label)):\n",
    "    if i < len(negative) :\n",
    "        data_label[i] = 1\n",
    "        \n",
    "\n",
    "# 数据随机划分train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(positive, data_label, test_size=0.2, random_state=42)\n",
    "print(\"train len:\", len(X_train))\n",
    "\n",
    "\n",
    "# 获取词袋模型训练数据\n",
    "X_train_bow = bow_data_prepare(X_train,vocab_to_int)\n",
    "X_test_bow = bow_data_prepare(X_test,vocab_to_int)\n",
    "\n",
    "\n",
    "# 构建TF-IDF模型的训练数据\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "\n",
    "X_train_tfidf = transformer.fit_transform(X_train_bow)\n",
    "X_test_tfidf = transformer.transform(X_test_bow)\n",
    "\n",
    "\n",
    "# 构建embed模型的训练数据\n",
    "X_train_embed = sent2id(X_train, 20)\n",
    "X_test_embed = sent2id(X_test, 20)\n",
    "\n",
    "print(\"all the preprocess done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,LSTM,TimeDistributed,Embedding,Dropout,Bidirectional,Dropout,Conv1D,MaxPooling1D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(vocab_to_int), input_length=20, trainable=True, output_dim=64))\n",
    "    model.add(Conv1D(input_shape=(20,64), filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    Adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,clipvalue=0.3)\n",
    "    model.compile(optimizer=Adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     \n",
    "def create_rnn_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(vocab_to_int), output_dim=64))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))#默认的merge形式为concat\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    Adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,clipvalue=0.5)\n",
    "    model.compile(optimizer=Adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, _X_train, _y_train, _X_test):\n",
    "    \n",
    "    oof_train = np.zeros((_X_train.shape[0], 2))\n",
    "#     oof_test = np.zeros((_y_train.shape[0]))\n",
    "    oof_test_skf = np.zeros((_X_test.shape[0], 2))\n",
    "\n",
    " #   oof_test_skf=np.empty((5,X_test.shape[0],2))\n",
    "#    oof_test_skf=np.empty((5,1))\n",
    "    \n",
    "    for i ,(train_index,test_index) in enumerate(kf.split(_X_train)):\n",
    "        print(i)\n",
    "        kf_X_train=_X_train[train_index]\n",
    "        kf_y_train=_y_train[train_index]\n",
    "        kf_X_test=_X_train[test_index]\n",
    "#         print(kf_X_test.shape)\n",
    "        \n",
    "        clf.fit(kf_X_train,kf_y_train)\n",
    "#         print(clf.predict_proba(kf_X_test).shape)\n",
    "\n",
    "#   把那个一折作为训练数据，循环之后会填满\n",
    "        # 得到每折测试集的结果\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)\n",
    "        oof_test_skf += 1 / 5 * clf.predict_proba(_X_test)#每一次的训练结果针对test的输出\n",
    "\n",
    "#     oof_test[:]= oof_test_skf.mean(axis=0)#去均值作为 test的结果\n",
    "    print(\"valid data shape:\",oof_train.shape,oof_test_skf.shape)\n",
    "    return oof_train ,oof_test_skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_nn(clf, _X_train, _y_train, _X_test):\n",
    "    \n",
    "    oof_train=np.zeros((_X_train.shape[0],2))\n",
    "#     oof_test=np.zeros((_y_train.shape))\n",
    "    oof_test_skf=np.zeros((_X_test.shape[0],2))\n",
    "    print(\"oof_train\",oof_train.shape)\n",
    "    \n",
    " #   oof_test_skf=np.empty((5,X_test.shape[0],2))\n",
    "#    oof_test_skf=np.empty((5,1))\n",
    "    \n",
    "    for i ,(train_index,test_index) in enumerate(kf.split(_X_train)):\n",
    "        print(i)\n",
    "        kf_X_train = _X_train[train_index]\n",
    "        kf_y_train = _y_train[train_index]\n",
    "        kf_X_test = _X_train[test_index]\n",
    "#         print(kf_X_test.shape)\n",
    "        t_label = keras.utils.to_categorical(kf_y_train, num_classes=2)\n",
    "    \n",
    "        clf.fit(kf_X_train, t_label)\n",
    "#         print(clf.predict_proba(kf_X_test).shape)\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)#把那个一折作为训练数据，循环之后会填满\n",
    "    \n",
    "        oof_test_skf += 1 / 5 * clf.predict_proba(_X_test)\n",
    "#     print(oof_test_skf)    \n",
    "#     oof_test[:]= oof_test_skf.mean(axis=0)#去均值作为 test的结果\n",
    "    print(\"valid data shape:\", oof_train.shape, oof_test_skf.shape)\n",
    "    return oof_train ,oof_test_skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 贝叶斯分类器\n",
    "clf_nb = MultinomialNB()\n",
    "\n",
    "# SVM分类器\n",
    "clf_svm = svm.SVC(cache_size=1000, kernel='linear', probability=True)\n",
    "\n",
    "# 构建RNN模型\n",
    "clf_rnn = KerasClassifier(build_fn = create_rnn_model, epochs=3, batch_size=1024)\n",
    "\n",
    "# 构建cnn模型\n",
    "clf_cnn = KerasClassifier(build_fn = create_cnn_model, epochs=3, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\dlnd\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 构建k折交叉验证\n",
    "kf = KFold(n_splits=5,random_state=2018)\n",
    "\n",
    "\n",
    "# 进行训练\n",
    "train_sets=[]\n",
    "test_sets=[]\n",
    "\n",
    "for clf in [clf_svm, clf_nb]:\n",
    "    train_set,test_set = get_oof(clf, X_train_tfidf, Y_train, X_test_tfidf)\n",
    "    print (len(train_set))\n",
    "    train_sets.append(train_set)\n",
    "    print (len(train_sets))\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "for clf in [clf_rnn, clf_cnn]:\n",
    "    train_set,test_set = get_oof_nn(clf, X_train_embed, Y_train, X_test_embed)\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91011094  0.08988906]\n",
      " [ 0.32061934  0.67938066]\n",
      " [ 0.89000793  0.10999207]\n",
      " ..., \n",
      " [ 0.47891341  0.52108659]\n",
      " [ 0.67980324  0.32019676]\n",
      " [ 0.55437774  0.44562226]]\n",
      "[[ 0.73668902  0.26331098]\n",
      " [ 0.22875531  0.77124469]\n",
      " [ 0.63645469  0.36354531]\n",
      " ..., \n",
      " [ 0.51651264  0.48348736]\n",
      " [ 0.59638107  0.40361893]\n",
      " [ 0.53337654  0.46662346]]\n",
      "[[ 0.96001935  0.03998067]\n",
      " [ 0.24489652  0.75510353]\n",
      " [ 0.95557177  0.04442823]\n",
      " ..., \n",
      " [ 0.61116683  0.38883322]\n",
      " [ 0.59094816  0.40905184]\n",
      " [ 0.40123555  0.59876448]]\n",
      "[[ 0.8874101   0.11258985]\n",
      " [ 0.25627583  0.74372417]\n",
      " [ 0.59583741  0.40416256]\n",
      " ..., \n",
      " [ 0.7275542   0.27244589]\n",
      " [ 0.67258638  0.32741368]\n",
      " [ 0.42781565  0.57218438]]\n",
      "(19561, 8) (4891, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\dlnd\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78675117562870578"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for result_set in train_sets:\n",
    "    print(result_set)\n",
    "\n",
    "\n",
    "# 按对应行对数组进行凭借\n",
    "meta_train = np.concatenate([result_set.reshape(-1,2) for result_set in train_sets], axis=1)\n",
    "meta_test = np.concatenate([y_test_set.reshape(-1,2) for y_test_set in test_sets], axis=1)\n",
    "\n",
    "\n",
    "print(meta_train.shape,meta_test.shape)\n",
    "\n",
    "clf_lr=LogisticRegression()\n",
    "clf_lr.fit(meta_train, Y_train)\n",
    "\n",
    "clf_lr.score(meta_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
